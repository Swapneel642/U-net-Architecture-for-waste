{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "JcdgZPlDEREk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zhjp8brt1123"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DoubleConv Module"
      ],
      "metadata": {
        "id": "9sDcl4HpEfiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "FySClGQ627sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unet Architecture"
      ],
      "metadata": {
        "id": "l39mEnhZEhTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Downsampling path\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Upsampling path\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)  # ConvTranspose2d\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            # Resize x to match skip_connection if needed\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](concat_skip)  # DoubleConv\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "rdtuMOTn4sjL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing of the input and output shape"
      ],
      "metadata": {
        "id": "lfsGRt5HELAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    x = torch.randn((3, 1, 161, 161))  # Batch size=3, 1-channel, 161x161 image\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {preds.shape}\")\n",
        "\n",
        "    assert preds.shape == x.shape, \"Output shape does not match input shape!\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXGjWU88MB0",
        "outputId": "d981dc8f-ebca-41e5-a855-0c7d0b5bd5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([3, 1, 161, 161])\n",
            "Output shape: torch.Size([3, 1, 161, 161])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "gOQqq1P2Ek1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(img_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "\n",
        "        # Normalize mask (convert 255 â†’ 1)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "0WnYdvW1-hYT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "glPA7GyPFoBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from dataset import SegmentationDataset\n",
        " from utils import (\n",
        "     load_checkpoint,\n",
        "     save_checkpoint,\n",
        "     get_loaders,\n",
        "     check_accuracy,\n",
        "     save_predictions_as_imgs,\n",
        " )\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 15\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_WIDTH = 1024  # Original: 3840\n",
        "IMAGE_HEIGHT = 1024  # Original: 2160\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# Paths (Update these before running)\n",
        "TRAIN_IMG_DIR =\n",
        "TRAIN_MASK_DIR =\n",
        "VAL_IMG_DIR =\n",
        "VAL_MASK_DIR =\n",
        "\n",
        "# Training function\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update tqdm loop\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    train_transform = A.Compose([\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Rotate(limit=35, p=1.0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    val_transforms = A.Compose([\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)  # Single Class Segmentation\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        TRAIN_IMG_DIR,\n",
        "        TRAIN_MASK_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        VAL_MASK_DIR,\n",
        "        BATCH_SIZE,\n",
        "        train_transform,\n",
        "        val_transforms,\n",
        "        NUM_WORKERS,\n",
        "        PIN_MEMORY,\n",
        "    )\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5LqEcqGuFpLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and check accuracy\n",
        "checkpoint = {\n",
        "\"state_dict\": model.state_dict(),\n",
        "\"optimizer\": optimizer.state_dict(),\n",
        "}\n",
        "save_checkpoint(checkpoint)\n",
        "\n",
        "# check_accuracy(val_loader, model, device=DEVICE)\n",
        "check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "# save_predictions_as_imgs(val_loader, model, folder=\"saved_predictions/\", device=DEVICE)\n",
        "save_predictions_as_imgs(val_loader, model, folder=\"saved_predictions/\", device=DEVICE)"
      ],
      "metadata": {
        "id": "VDcXqKQIMP2u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils file"
      ],
      "metadata": {
        "id": "O9fMvc2EPDdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import SegmentationDataset\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    \"\"\"Saves model checkpoint.\"\"\"\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    \"\"\"Loads model checkpoint.\"\"\"\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir, train_maskdir, val_dir, val_maskdir, batch_size,\n",
        "    train_transform, val_transform, num_workers=4, pin_memory=True\n",
        "):\n",
        "    \"\"\"Creates and returns data loaders for training and validation.\"\"\"\n",
        "\n",
        "    train_ds = SegmentationDataset(\n",
        "        image_dir=train_dir,\n",
        "        mask_dir=train_maskdir,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_ds = SegmentationDataset(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    \"\"\"Checks model accuracy and Dice score on the dataset.\"\"\"\n",
        "\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)  # Ensure proper shape\n",
        "\n",
        "            preds = torch.sigmoid(model(x))  # Apply sigmoid activation\n",
        "            preds = (preds > 0.5).float()  # Convert to binary mask\n",
        "\n",
        "            num_correct += (preds == y).sum().item()\n",
        "            num_pixels += preds.numel()\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
        "\n",
        "    acc = (num_correct / num_pixels) * 100\n",
        "    print(f\"Got {num_correct}/{num_pixels} pixels correct with accuracy {acc:.2f}%\")\n",
        "    print(f\"Dice score: {dice_score / len(loader):.4f}\")\n",
        "\n",
        "    model.train()  # Set back to training mode\n",
        "\n",
        "    return acc, dice_score / len(loader)\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
        "    \"\"\"Saves model predictions as images.\"\"\"\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/{idx}.png\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "eryVj1MUMPyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITugk3mcMPYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}