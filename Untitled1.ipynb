{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOmsSWTyA/5AQPywSuXxM/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swapneel642/U-net-Architecture-for-waste/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogEJPU1O9-Fa",
        "outputId": "0b010a3f-36b3-4023-a18b-6e73449ac12a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for training and validation\n",
        "TRAIN_IMG_DIR = '/content/drive/MyDrive/U-Net_Dataset/train_image'\n",
        "TRAIN_MASK_DIR = '/content/drive/MyDrive/U-Net_Dataset/train_mask'\n",
        "VAL_IMG_DIR = '/content/drive/MyDrive/U-Net_Dataset/val_image'\n",
        "VAL_MASK_DIR = '/content/drive/MyDrive/U-Net_Dataset/val_mask'"
      ],
      "metadata": {
        "id": "bf1GDy4Y9-Ap"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ho_nQG_B994J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Library"
      ],
      "metadata": {
        "id": "FdlxY0hz18xh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ncQrtbl-11Ek"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unet Model"
      ],
      "metadata": {
        "id": "25EAe7Bb16fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Downsampling path\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Upsampling path\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)  # ConvTranspose2d\n",
        "            skip_connection = skip_connections[idx // 2]\n",
        "\n",
        "            # Resize x to match skip_connection if needed\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = F.interpolate(x, size=skip_connection.shape[2:], mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx + 1](concat_skip)  # DoubleConv\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "fVGDIAQV12vx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset Class"
      ],
      "metadata": {
        "id": "AXZgS5Nt2XjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(img_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
        "\n",
        "        # Ensure mask filename matches the pattern 'frame_XXXXXXXX_Waste.png'\n",
        "        mask_filename = self.images[idx].replace(\".png\", \"_Waste.png\")\n",
        "        mask_path = os.path.join(self.mask_dir, mask_filename)\n",
        "\n",
        "        # Load the image and mask\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "\n",
        "        # Normalize mask (convert 255 â†’ 1)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "t5-hq6Qn2BW6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations"
      ],
      "metadata": {
        "id": "R7aNhG-e2il4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# Define transformations\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "RiSPtpav2R2Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data Loaders"
      ],
      "metadata": {
        "id": "gvHrrL_H3BC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_loaders(train_img_dir, train_mask_dir, val_img_dir, val_mask_dir, batch_size, train_transform, val_transform, num_workers=2, pin_memory=True):\n",
        "    train_ds = SegmentationDataset(img_dir=train_img_dir, mask_dir=train_mask_dir, transform=train_transform)\n",
        "    val_ds = SegmentationDataset(img_dir=val_img_dir, mask_dir=val_mask_dir, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "QGtfIbmN2lbg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DEVICE before initializing the model\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize model\n",
        "model = UNET(in_channels=3, out_channels=1).to(DEVICE)"
      ],
      "metadata": {
        "id": "rxBTUorE6xUB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the Training Function"
      ],
      "metadata": {
        "id": "u-LUxM7O8RbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    model.train()  # Set model to training mode\n",
        "    loop = tqdm(loader, leave=True)  # Progress bar\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)  # Add channel dimension\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "kW_3x-3B8JFc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Model Evaluation"
      ],
      "metadata": {
        "id": "zDYfQvmV8YzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    \"\"\"Checks model accuracy and Dice score on the dataset.\"\"\"\n",
        "\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)  # Ensure proper shape\n",
        "\n",
        "            preds = torch.sigmoid(model(x))  # Apply sigmoid activation\n",
        "            preds = (preds > 0.5).float()  # Convert to binary mask\n",
        "\n",
        "            num_correct += (preds == y).sum().item()\n",
        "            num_pixels += preds.numel()\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
        "\n",
        "    acc = (num_correct / num_pixels) * 100\n",
        "    print(f\" Validation Accuracy: {acc:.2f}%\")\n",
        "    print(f\" Dice Score: {dice_score / len(loader):.4f}\")\n",
        "\n",
        "    model.train()  # Switch back to training mode\n",
        "    return acc, dice_score / len(loader)"
      ],
      "metadata": {
        "id": "X4RYXKJH8U0K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Model Checkpoints"
      ],
      "metadata": {
        "id": "LSE3-Wc48n-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    \"\"\"Saves model checkpoint.\"\"\"\n",
        "    print(\" Saving checkpoint...\")\n",
        "    torch.save(state, filename)\n",
        "    print(\" Checkpoint saved!\")"
      ],
      "metadata": {
        "id": "ueuE9Fm68caB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model Checkpoints"
      ],
      "metadata": {
        "id": "_hcXgJoH8rvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint, model):\n",
        "    \"\"\"Loads model checkpoint.\"\"\"\n",
        "    print(\"Loading checkpoint...\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    print(\"Checkpoint loaded!\")"
      ],
      "metadata": {
        "id": "BwQ-cKTn8p9y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "ERiKjKZy84tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler, device=\"cuda\"):\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device)\n",
        "        targets = targets.float().unsqueeze(1).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update tqdm progress bar\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "THBjSTiv8x8C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Accuracy & Dice Score Calculation"
      ],
      "metadata": {
        "id": "ZEK_0ukt9E2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    \"\"\"Evaluates model accuracy and Dice score on a dataset.\"\"\"\n",
        "\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)  # Ensure proper shape\n",
        "\n",
        "            preds = torch.sigmoid(model(x))  # Apply sigmoid activation\n",
        "            preds = (preds > 0.5).float()  # Convert to binary mask\n",
        "\n",
        "            num_correct += (preds == y).sum().item()\n",
        "            num_pixels += preds.numel()\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
        "\n",
        "    acc = (num_correct / num_pixels) * 100\n",
        "    dice = dice_score / len(loader)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.2f}%\")\n",
        "    print(f\"Dice Score: {dice:.4f}\")\n",
        "\n",
        "    model.train()  # Switch back to training mode\n",
        "\n",
        "    return acc, dice"
      ],
      "metadata": {
        "id": "wXZcEwwm9B2Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model Predictions as Images"
      ],
      "metadata": {
        "id": "T9coRqlL9Mc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
        "    \"\"\"Saves model predictions as images for visualization.\"\"\"\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()  # Convert to binary mask\n",
        "\n",
        "        # Save predicted mask\n",
        "        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
        "        # Save ground truth mask for comparison\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/gt_{idx}.png\")\n",
        "\n",
        "    print(f\"Saved predictions in '{folder}'\")\n",
        "    model.train()  # Switch back to training mode"
      ],
      "metadata": {
        "id": "XSD7--238715"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "Y2FIR9__9UZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(loader, model, optimizer, loss_fn, scaler, device=\"cuda\"):\n",
        "    \"\"\"Trains the model for one epoch on the given data loader.\"\"\"\n",
        "\n",
        "    loop = tqdm(loader, leave=True)  # Progress bar\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device)\n",
        "        targets = targets.float().unsqueeze(1).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update tqdm loop with loss value\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "YZcHB_Jn9QZL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training Loop"
      ],
      "metadata": {
        "id": "Gid27hr79epB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DEVICE before initializing the model\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize model\n",
        "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 15\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_WIDTH = 256\n",
        "IMAGE_HEIGHT = 256\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# Create Data Loaders\n",
        "train_loader, val_loader = get_loaders(\n",
        "    TRAIN_IMG_DIR, TRAIN_MASK_DIR, VAL_IMG_DIR, VAL_MASK_DIR,\n",
        "    BATCH_SIZE, NUM_WORKERS, PIN_MEMORY\n",
        ")\n",
        "\n",
        "# Define Loss Function & Optimizer\n",
        "loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Define GradScaler for mixed precision training\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training loop to train U-Net model for multiple epochs.\"\"\"\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"\\nðŸ”¹ Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "        # Check accuracy every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            check_accuracy(val_loader, model, DEVICE)\n",
        "\n",
        "        # Save model checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            save_checkpoint({\"state_dict\": model.state_dict()}, filename=f\"checkpoint_epoch_{epoch+1}.pth.tar\")\n",
        "\n",
        "# Run training\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "b320-pCeCBM6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}